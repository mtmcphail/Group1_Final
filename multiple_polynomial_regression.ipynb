{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bottom-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sqlalchemy import create_engine, func\n",
    "import psycopg2\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "coral-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to Google Cloud Instance\n",
    "#format is //user:password@sql instance public IP address/database name\n",
    "db_string = f\"postgres://postgres:alcohol-ca@35.194.17.20/alcohol_cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "integral-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create engine\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fluid-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import one of the following tables \n",
    "# alcohol_ca (very big messy merged table)\n",
    "# alcohol_ca_clean (coming soon after data cleanup)\n",
    "# alcohol_qrt_mile (original raw dataset)\n",
    "# personal_income\n",
    "# typology_ca\n",
    "# unemployment\n",
    "#df = pd.read_sql_table('alcohol_ca_clean',engine)\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "russian-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60210 entries, 0 to 60209\n",
      "Data columns (total 27 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   Unnamed: 0                     60210 non-null  int64  \n",
      " 1   county_name                    60210 non-null  object \n",
      " 2   geotype                        60210 non-null  object \n",
      " 3   geoname                        60210 non-null  object \n",
      " 4   race_eth_updt                  60210 non-null  object \n",
      " 5   num_ppl_qrt_race               60210 non-null  int64  \n",
      " 6   tot_ppl_region_race            60210 non-null  int64  \n",
      " 7   tot_people_qrt                 60210 non-null  float64\n",
      " 8   tot_people_region              60210 non-null  float64\n",
      " 9   pct_ppl_qrt_race               60210 non-null  float64\n",
      " 10  pct_all_qrt_race               60210 non-null  float64\n",
      " 11  pct_all_ppl_qrt_race           60210 non-null  float64\n",
      " 12  region_name                    60210 non-null  object \n",
      " 13  per_capita_income              60210 non-null  int64  \n",
      " 14  total_income                   60210 non-null  int64  \n",
      " 15  population                     60210 non-null  int64  \n",
      " 16  unemp_2014                     60210 non-null  float64\n",
      " 17  median_house_price             60210 non-null  int64  \n",
      " 18  median_house_pct               60210 non-null  float64\n",
      " 19  metro_status                   60210 non-null  int64  \n",
      " 20  economic_type_label            60210 non-null  object \n",
      " 21  low_education_2015             60210 non-null  int64  \n",
      " 22  low_employment_cnty            60210 non-null  int64  \n",
      " 23  pop_loss_2010                  60210 non-null  int64  \n",
      " 24  retirement_dest_2015           60210 non-null  int64  \n",
      " 25  persistent_poverty_2013        60210 non-null  int64  \n",
      " 26  persistent_child_poverty_2013  60210 non-null  int64  \n",
      "dtypes: float64(7), int64(14), object(6)\n",
      "memory usage: 12.4+ MB\n"
     ]
    }
   ],
   "source": [
    "### Or read in CSV\n",
    "df = pd.read_csv('alcohol_ca_clean_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "secure-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Training and Testing\n",
    "# Create our features\n",
    "#y = pd.DataFrame(df['pct_all_qrt_race'])\n",
    "#X = pd.get_dummies(df[['geotype','race_eth_updt','per_capita_income','total_income','population','unemp_2014','median_house_price','median_house_pct','metro_status','economic_type_label','low_education_2015','low_employment_cnty','pop_loss_2010','retirement_dest_2015','persistent_poverty_2013','persistent_child_poverty_2013']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "focused-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Station\n",
    "#Split the Data into Training and Testing\n",
    "# Create our features\n",
    "y = pd.DataFrame(df['pct_all_qrt_race'])\n",
    "X = pd.get_dummies(df[['geotype','race_eth_updt','total_income','population','unemp_2014','median_house_price','metro_status','economic_type_label','low_education_2015','low_employment_cnty','pop_loss_2010']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "efficient-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bored-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the X Features\n",
    "\n",
    "# Create the scaler instance\n",
    "X_scaler = StandardScaler()\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "# Scale the features data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "recent-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolynomialFeatures (prepreprocessing)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.fit_transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "vocational-pillow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Logistic Regression model \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(X_train_poly, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fresh-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "y_predict = model.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "stylish-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02516998247542259\n"
     ]
    }
   ],
   "source": [
    "# what is the mean squared error?\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "warming-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5426349523462342 0.5333338735571798\n"
     ]
    }
   ],
   "source": [
    "# compute with sklearn linear_model, although could not find any function to compute adjusted-r-square directly from documentation\n",
    "print(model.score(X_test_poly, y_test), 1 - (1-model.score(X_test_poly, y_test))*(len(y_test)-1)/(len(y_test)-X_test_poly.shape[1]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-investment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-shirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-stone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-placement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
